{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This notebook walks your through the process of creating clips with LLM prompts using multimodal information of video. \n",
    "\n",
    "Pick a video, decide your prompt, generate a new clip âš¡ï¸\n",
    "\n",
    "It's as simple as it sounds.\n",
    "\n",
    "If you want to go extra mile you can add Image Overlays or Audio overlays on these clips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But first, let's install the dependecies.\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading The Video\n",
    "\n",
    "Before proceeding, ensure access to [VideoDB](https://videodb.io) API key. If not, sign up for API access on the respective platforms.\n",
    "\n",
    "> Get your API key from [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required** ) ðŸŽ‰\n",
    "\n",
    "You can either source a new video from YouTube or select an existing one from your VideoDB collection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import videodb\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# TODO: setup .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Connect to VideoDB\n",
    "conn = videodb.connect()\n",
    "coll = conn.get_collection()\n",
    "\n",
    "# TODO: Add video_id if video already exists in the collection\n",
    "video_id = os.getenv(\"MULTIMODAL_DEMO_VIDEO_ID\")\n",
    "video_url = \"https://www.youtube.com/watch?v=NZGLHdcw2RM\"\n",
    "\n",
    "if not video_id:\n",
    "    video = coll.upload(url=video_url)\n",
    "else:\n",
    "    video = coll.get_video(video_id)\n",
    "\n",
    "print(f\"video_id: {video.id}, name: {video.name}\")\n",
    "video.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing The Visual And Spoken Information\n",
    "\n",
    "In this example, we are using a cricket explainer video. We will index it based on spoken words and scenes to support complex queries that require multimodal information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell will retrieve the transcript if it's already indexed; otherwise, it will first index the content and then set the transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    transcript = video.get_transcript()\n",
    "    transcript_text = video.get_transcript_text()\n",
    "except Exception:\n",
    "    video.index_spoken_words()\n",
    "    transcript = video.get_transcript()\n",
    "    transcript_text = video.get_transcript_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can either provide the `scene_index_id` of the video, if available, or leave it blank to index the video for visual retrieval.\n",
    "\n",
    "To know more about scene indexing click [here](https://docs.videodb.io/scene-index-guide-80)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add scene_index_id here if already indexed.\n",
    "scene_index_id = os.getenv(\"MULTIMODAL_DEMO_SCENE_INDEX_ID\")\n",
    "\n",
    "if not scene_index_id:\n",
    "    scene_index_id = video.index_scenes(\n",
    "        prompt=\"Summarize the essence of the scene in one or two concise sentences.\"\n",
    "    )\n",
    "scenes = video.get_scene_index(scene_index_id)\n",
    "print(f\"Video is indexed with scene_index_id {scene_index_id} and has {len(scenes)} scenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting The Indexed Scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scene in scenes:\n",
    "    print(f\"{scene['start']}-{scene['end']}: {scene['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Results From Your Prompt\n",
    "\n",
    "The `multimodal_promper` function in `video_prompter.py` takes the transcript and indexed scenes, combines them, chunks them, and then parallelly calls the LLM with user and system prompts to retrieve the desired matching scenes.\n",
    "\n",
    "To create a clip using the `multimodal_promper` function from a video, it's crucial to craft a specific prompt that will help identify the most relevant segments for your use case. This prompt should highlight the themes, activity, or combination of verbal or visual cues you're interested in. \n",
    "\n",
    "The `multimodal_promper` will return sentences which are visual description from matched chunks. Review these to ensure they align with your needs. You can then use these segments to create your clip, focusing on the content that's most relevant to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_prompter import multimodal_prompter\n",
    "\n",
    "user_prompt = \"find the scene explaining the cricket rules using infographics, remember rules with infographics.\"\n",
    "# Here, we are only interested in the section of the video where cricket rules are explained using infographics.\n",
    "# If we create a clip using only the spoken index data, we won't know where the infographics are.\n",
    "# If we create a clip using only the visual index data, we may include additional infographics that aren't actually about the rules but might appear to be rule explanations based on visual information.\n",
    "# By creating a clip using combined data, we achieve a much more precise intersection where the infographics are present, and the rules are being explained.\n",
    "result = multimodal_prompter(transcript, scenes, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate The Clip\n",
    "To generate a clip, first we'll call `get_result_timestamps` from `video_prompter.py` it uses VideoDB's `keyword search` feature. We already leveraged the power of the LLM (Large Language Model) to identify relevant sentences. We'll use the search results to create a programmable video stream. Here's how you can approach this process:\n",
    "\n",
    "We have the descriptions in the `results` variable. We input these keywords into VideoDB's keyword search feature. This function will search through the indexed scenes of your videos to find matches.\n",
    "\n",
    "The search will return a SearchResult object, which contains detailed information about the found segments, including their timestamps, the text of the scene description.\n",
    "\n",
    "**Create a Programmable Video Stream with Timeline:** With the specific segments identified, you can now use `build_video_timeline` from `video_prompter.py` to get the Timeline to create a new programmable video stream. The Timeline tool allows you to stitch together video segments based on the timestamps provided in the SearchResult object. You can arrange, cut, or combine these segments to craft a fresh video stream that focuses on your topic of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import play_stream\n",
    "from videodb.timeline import Timeline\n",
    "\n",
    "from video_prompter import get_result_timestamps, build_video_timeline\n",
    "\n",
    "timeline = Timeline(conn)\n",
    "result_timestamps = get_result_timestamps(video, result, scene_index_id=scene_index_id)\n",
    "timeline, duration = build_video_timeline(video, result_timestamps, timeline)\n",
    "stream = timeline.generate_stream()\n",
    "play_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus - With Single Modality For Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip Using Only Spoken Information\n",
    "Since the transcript does not include information about infographics, not all infographics with rules are captured in the result.\n",
    "Segments explaining how many days each format is played are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_prompter import text_prompter\n",
    "\n",
    "user_prompt = \"find the scene explaining the cricket rules using infographics, remember rules with infographics.\"\n",
    "text_result = text_prompter(transcript_text, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = Timeline(conn)\n",
    "result_timestamps = get_result_timestamps(video, text_result, index_type=\"spoken_word\")\n",
    "timeline, duration  = build_video_timeline(video, result_timestamps, timeline)\n",
    "stream = timeline.generate_stream()\n",
    "play_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model doesn't have information about the visual infographics, the segments toward the end, where the rules about overs are explained, are missing from the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clip On Scene Information\n",
    "\n",
    "Here, segments that contain infographics but are not related to rules are also being captured in the results too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  video_prompter import scene_prompter\n",
    "\n",
    "user_prompt = \"find the scene explaining the cricket rules using infographics, remember rules with infographics.\"\n",
    "scene_result = scene_prompter(scenes, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = Timeline(conn)\n",
    "result_timestamps = get_result_timestamps(video, scene_result, scene_index_id=scene_index_id)\n",
    "timeline, duration = build_video_timeline(video, result_timestamps, timeline)\n",
    "stream = timeline.generate_stream()\n",
    "play_stream(stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model doesn't have information about the spoken content, false positive segments are being addedâ€”these segments aren't actually about rules but may appear to be based on visual information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion \n",
    "\n",
    "As shown above, in cases where both modalities are important to retrieve the desired clip, this notebook works best. If only one modality is sufficient, the respective functions can be used.\n",
    "\n",
    "We can't wait to see which approach works best for your videosâ€”do share your results with us!\n",
    "\n",
    "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
    "* [GitHub](https://github.com/video-db)\n",
    "* [Website](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fvideodb.io)\n",
    "* [Email](ashu@videodb.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground For You To Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url = \"\"\n",
    "video_id = \"\"\n",
    "\n",
    "if not video_id:\n",
    "    video = coll.upload(url=video_url)\n",
    "else:\n",
    "    video = coll.get_video(video_id)\n",
    "\n",
    "print(f\"video_id: {video.id}, name: {video.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    transcript = video.get_transcript()\n",
    "    transcript_text = video.get_transcript_text()\n",
    "except Exception:\n",
    "    video.index_spoken_words()\n",
    "    transcript = video.get_transcript()\n",
    "    transcript_text = video.get_transcript_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_index_id = \"\"\n",
    "\n",
    "if not scene_index_id:\n",
    "    scene_index_id = video.index_scenes(\n",
    "        prompt=\"Summarize the essence of the scene in one or two concise sentences.\"\n",
    "    )\n",
    "scenes = video.get_scene_index(scene_index_id)\n",
    "print(f\"Video is indexed with scene_index_id {scene_index_id} and has {len(scenes)} scenes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video_prompter import multimodal_prompter\n",
    "\n",
    "user_prompt = \"\"\n",
    "result = multimodal_prompter(transcript, scenes, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import play_stream\n",
    "from videodb.timeline import Timeline\n",
    "\n",
    "from video_prompter import get_result_timestamps, build_video_timeline\n",
    "\n",
    "timeline = Timeline(conn)\n",
    "result_timestamps = get_result_timestamps(video, result, scene_index_id=scene_index_id)\n",
    "timeline, duration = build_video_timeline(video, result_timestamps, timeline)\n",
    "stream = timeline.generate_stream()\n",
    "play_stream(stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
